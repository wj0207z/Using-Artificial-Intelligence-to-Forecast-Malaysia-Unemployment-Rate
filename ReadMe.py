import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime

# Page configuration
st.set_page_config(
    page_title="Read Me - Methodology & Models",
    page_icon="üìö",
    layout="wide"
)

# Header
st.title("üìö Read Me - Methodology & Models")
st.markdown("---")

# Table of Contents
st.sidebar.title("üìã Table of Contents")
st.sidebar.markdown("""
- [üìä Data Overview](#data-overview)
- [üî¨ Methodology](#methodology)
- [üìà Data Preprocessing](#data-preprocessing)
- [ü§ñ Model Training Process](#model-training-process)
- [üìä Statistical Models](#statistical-models)
- [üß† Deep Learning Models](#deep-learning-models)
- [üìä Model Comparison](#model-comparison)
- [üîç Evaluation Metrics](#evaluation-metrics)
- [üí° Best Practices](#best-practices)
- [üìö References](#references)
""")

# Data Overview Section
st.header("üìä Data Overview")
st.markdown("""
### Dataset Description
This application uses **Malaysia Quarterly Labour Force Survey Data** containing comprehensive employment statistics from 2010 to 2024.

### Available Metrics
The dataset includes the following key indicators:
- **Labour Force (lf)**: Total number of people available for work (in thousands)
- **Employed (lf_employed)**: Number of people currently employed (in thousands)
- **Unemployed (lf_unemployed)**: Number of people actively seeking employment (in thousands)
- **Outside Labour Force (lf_outside)**: People not in the labour force (in thousands)
- **Participation Rate (%) (p_rate)**: Percentage of working-age population in labour force
- **Employment to Population Ratio (%) (ep_ratio)**: Percentage of working-age population employed
- **Unemployment Rate (%) (u_rate)**: Percentage of labour force that is unemployed

### Data Characteristics
- **Frequency**: Quarterly (4 observations per year)
- **Time Span**: 2010-2024 (approximately 60 quarters)
- **Data Points**: 60 quarterly observations
- **Seasonality**: Strong quarterly patterns due to economic cycles
- **Trend**: Long-term economic trends and structural changes
- **Units**: Labour force numbers in thousands, rates in percentages
""")

# Methodology Section
st.header("üî¨ Methodology")
st.markdown("""
### Forecasting Approach
This application implements a **comprehensive forecasting methodology** that combines statistical and machine learning approaches to provide robust unemployment rate predictions.

### Key Principles
1. **Multi-Model Ensemble**: Using multiple models to capture different aspects of the data
2. **Train-Test Split**: Consistent 80-20 split for model validation
3. **Cross-Validation**: Ensuring model reliability through proper validation
4. **Residual Analysis**: Comprehensive diagnostic testing for model adequacy
5. **Explainability**: Providing insights into model decisions and feature importance

### Methodology Flow
```
Data Collection ‚Üí Preprocessing ‚Üí Model Selection ‚Üí Training ‚Üí Validation ‚Üí Forecasting ‚Üí Evaluation
```
""")

# Data Preprocessing Section
st.header("üìà Data Preprocessing")
st.markdown("""
### Step 1: Data Loading and Cleaning
```python
# Load dataset
df = pd.read_csv("MalaysiaQuarterlyLabourForce.csv")
df['date'] = pd.to_datetime(df['date'])
df = df.sort_values('date')
df.set_index('date', inplace=True)

# Handle missing values
series = df[selected_metric].dropna()
```

### Step 2: Data Quality Assessment
- **Missing Values**: Identified and removed any incomplete observations
- **Outliers**: Detected and handled extreme values
- **Consistency**: Ensured data format consistency across time periods
- **Stationarity**: Tested for stationarity using Augmented Dickey-Fuller test

### Step 3: Feature Engineering
- **Lag Features**: Created lagged variables for time series models
- **Seasonal Decomposition**: Extracted trend, seasonal, and residual components
- **Scaling**: Applied Min-Max scaling for neural network models
- **Window Creation**: Generated sliding windows for deep learning models

### Step 4: Train-Test Split
```python
# Consistent 80-20 split across all models
test_pct = 20  # 20% for testing
test_size = int(len(series) * test_pct / 100)
train_size = len(series) - test_size

train_series = series.iloc[:train_size]
test_series = series.iloc[train_size:]
```
""")

# Model Training Process Section
st.header("ü§ñ Model Training Process")
st.markdown("""
### Training Strategy
Each model follows a systematic training process designed to ensure optimal performance and prevent overfitting.

### Common Training Steps
1. **Parameter Initialization**: Set initial hyperparameters based on data characteristics
2. **Model Fitting**: Train the model on the training dataset
3. **Validation**: Use validation set to monitor performance
4. **Hyperparameter Tuning**: Optimize parameters based on validation performance
5. **Final Training**: Retrain with optimal parameters on full training set
6. **Testing**: Evaluate on held-out test set

### Model-Specific Training Processes

#### Statistical Models (ARIMA, SARIMA, Exponential Smoothing)
```python
# Auto ARIMA - Automatic parameter selection
model = pm.auto_arima(
    train_series,
    seasonal=force_seasonal,
    m=4,  # Quarterly seasonality
    max_d=1,  # Maximum differencing
    stepwise=True,
    suppress_warnings=True
)

# Exponential Smoothing - Automatic method selection
model = ExponentialSmoothing(
    series, 
    trend='add', 
    seasonal='add', 
    seasonal_periods=4
)
fitted_model = model.fit()
```

#### Deep Learning Models (LSTM, GRU, RNN, CNN)
```python
# Data preparation for neural networks
scaler = MinMaxScaler()
scaled_series = scaler.fit_transform(series.values.reshape(-1, 1)).flatten()

# Create lagged sequences
def create_lagged_sequences(series, n_lags):
    X, y = [], []
    for i in range(n_lags, len(series)):
        X.append(series[i-n_lags:i])
        y.append(series[i])
    return np.array(X), np.array(y)

# Model architecture
model = Sequential([
    LSTM(n_units, input_shape=(n_lags, 1)),
    Dropout(dropout),
    Dense(1)
])

# Training with early stopping
es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
history = model.fit(
    X_train, y_train,
    epochs=epochs,
    batch_size=batch_size,
    validation_split=val_split,
    callbacks=[es],
    verbose=0
)
```

### Validation Strategy
- **Time Series Cross-Validation**: Forward chaining validation
- **Early Stopping**: Prevent overfitting in neural networks
- **Residual Analysis**: Comprehensive diagnostic testing
- **Out-of-Sample Testing**: Evaluate on unseen data
""")

# Statistical Models Section
st.header("üìä Statistical Models")

## ARIMA Model
st.subheader("üìà ARIMA (AutoRegressive Integrated Moving Average)")
st.markdown("""
### Model Description
ARIMA models capture temporal dependencies through autoregressive (AR) and moving average (MA) components, with differencing (I) to achieve stationarity.

### Mathematical Formulation
**ARIMA(p,d,q) Model:**
```
(1 - œÜ‚ÇÅB - œÜ‚ÇÇB¬≤ - ... - œÜ‚ÇöB·µñ)(1 - B)·µàY‚Çú = (1 + Œ∏‚ÇÅB + Œ∏‚ÇÇB¬≤ + ... + Œ∏‚ÇöB·µñ)Œµ‚Çú
```

Where:
- **p**: AR order (number of past values)
- **d**: Differencing order (trend removal)
- **q**: MA order (number of past errors)
- **œÜ**: AR coefficients
- **Œ∏**: MA coefficients
- **B**: Backshift operator
- **Œµ‚Çú**: White noise error

### Training Process
1. **Stationarity Test**: Augmented Dickey-Fuller test
2. **Differencing**: Apply if non-stationary
3. **Parameter Selection**: Auto ARIMA with AIC/BIC optimization
4. **Model Fitting**: Maximum likelihood estimation
5. **Diagnostic Testing**: Residual analysis

### Key Features
- ‚úÖ Automatic parameter selection
- ‚úÖ Handles trends and seasonality
- ‚úÖ Interpretable parameters
- ‚úÖ Confidence intervals
- ‚ö†Ô∏è Assumes linear relationships
- ‚ö†Ô∏è Requires stationarity
""")

## SARIMA Model
st.subheader("üåä SARIMA (Seasonal ARIMA)")
st.markdown("""
### Model Description
SARIMA extends ARIMA to handle seasonal patterns by including seasonal autoregressive, differencing, and moving average components.

### Mathematical Formulation
**SARIMA(p,d,q)(P,D,Q,m) Model:**
```
Œ¶(B·µê)(1 - œÜ‚ÇÅB - œÜ‚ÇÇB¬≤ - ... - œÜ‚ÇöB·µñ)(1 - B)·µà(1 - B·µê)·¥∞Y‚Çú = Œò(B·µê)(1 + Œ∏‚ÇÅB + Œ∏‚ÇÇB¬≤ + ... + Œ∏‚ÇöB·µñ)Œµ‚Çú
```

Where:
- **P**: Seasonal AR order
- **D**: Seasonal differencing order
- **Q**: Seasonal MA order
- **m**: Seasonal period (4 for quarterly data)
- **Œ¶**: Seasonal AR coefficients
- **Œò**: Seasonal MA coefficients

### Training Process
1. **Seasonality Detection**: Seasonal decomposition
2. **Seasonal Differencing**: Remove seasonal trends
3. **Parameter Selection**: Grid search for optimal parameters
4. **Model Fitting**: Maximum likelihood estimation
5. **Validation**: Out-of-sample testing

### Key Features
- ‚úÖ Captures seasonal patterns
- ‚úÖ Handles multiple time scales
- ‚úÖ Automatic seasonal detection
- ‚úÖ Robust for seasonal data
- ‚ö†Ô∏è More complex than ARIMA
- ‚ö†Ô∏è Requires sufficient seasonal data
""")

## Exponential Smoothing
st.subheader("üìà Exponential Smoothing")
st.markdown("""
### Model Description
Exponential smoothing uses weighted averages of past observations, with weights decreasing exponentially for older observations.

### Mathematical Formulation
**Holt-Winters Model:**
```
Level: l‚Çú = Œ±y‚Çú + (1-Œ±)(l‚Çú‚Çã‚ÇÅ + b‚Çú‚Çã‚ÇÅ)
Trend: b‚Çú = Œ≤(l‚Çú - l‚Çú‚Çã‚ÇÅ) + (1-Œ≤)b‚Çú‚Çã‚ÇÅ
Seasonal: s‚Çú = Œ≥(y‚Çú - l‚Çú) + (1-Œ≥)s‚Çú‚Çã‚Çò
Forecast: ≈∑‚Çú‚Çä‚Çï = l‚Çú + h¬∑b‚Çú + s‚Çú‚Çä‚Çï‚Çã‚Çò
```

Where:
- **Œ±**: Level smoothing parameter
- **Œ≤**: Trend smoothing parameter
- **Œ≥**: Seasonal smoothing parameter
- **m**: Seasonal period

### Training Process
1. **Method Selection**: Auto-select best method (Simple/Holt/Holt-Winters)
2. **Parameter Optimization**: Minimize AIC/BIC
3. **Model Fitting**: Maximum likelihood estimation
4. **Validation**: Cross-validation

### Key Features
- ‚úÖ Simple and interpretable
- ‚úÖ Automatic method selection
- ‚úÖ Handles trends and seasonality
- ‚úÖ Robust to outliers
- ‚ö†Ô∏è Assumes additive seasonality
- ‚ö†Ô∏è Limited to linear trends
""")

# Deep Learning Models Section
st.header("üß† Deep Learning Models")

## LSTM Model
st.subheader("üîÆ LSTM (Long Short-Term Memory)")
st.markdown("""
### Model Description
LSTM networks are designed to capture long-term dependencies in sequential data through specialized memory cells and gating mechanisms.

### Architecture Components
```
Input Gate: i‚Çú = œÉ(W·µ¢[h‚Çú‚Çã‚ÇÅ, x‚Çú] + b·µ¢)
Forget Gate: f‚Çú = œÉ(Wf[h‚Çú‚Çã‚ÇÅ, x‚Çú] + bf)
Output Gate: o‚Çú = œÉ(W‚Çí[h‚Çú‚Çã‚ÇÅ, x‚Çú] + b‚Çí)
Cell State: CÃÉ‚Çú = tanh(Wc[h‚Çú‚Çã‚ÇÅ, x‚Çú] + bc)
Memory Update: C‚Çú = f‚Çú ‚äô C‚Çú‚Çã‚ÇÅ + i‚Çú ‚äô CÃÉ‚Çú
Hidden State: h‚Çú = o‚Çú ‚äô tanh(C‚Çú)
```

### Training Process
1. **Data Preparation**: Create lagged sequences
2. **Scaling**: Min-Max normalization
3. **Architecture**: LSTM + Dropout + Dense layers
4. **Training**: Adam optimizer with early stopping
5. **Validation**: Monitor validation loss

### Key Features
- ‚úÖ Captures long-term dependencies
- ‚úÖ Handles vanishing gradient problem
- ‚úÖ Non-linear pattern recognition
- ‚úÖ Robust to noise
- ‚ö†Ô∏è Requires more data
- ‚ö†Ô∏è Computationally intensive
""")

## GRU Model
st.subheader("ü§ñ GRU (Gated Recurrent Unit)")
st.markdown("""
### Model Description
GRU is a simplified version of LSTM with fewer parameters but similar performance, using update and reset gates.

### Architecture Components
```
Update Gate: z‚Çú = œÉ(Wz[h‚Çú‚Çã‚ÇÅ, x‚Çú] + bz)
Reset Gate: r‚Çú = œÉ(Wr[h‚Çú‚Çã‚ÇÅ, x‚Çú] + br)
Candidate: hÃÉ‚Çú = tanh(Wh[r‚Çú ‚äô h‚Çú‚Çã‚ÇÅ, x‚Çú] + bh)
Hidden State: h‚Çú = (1 - z‚Çú) ‚äô h‚Çú‚Çã‚ÇÅ + z‚Çú ‚äô hÃÉ‚Çú
```

### Training Process
1. **Data Preparation**: Same as LSTM
2. **Architecture**: GRU + Dropout + Dense layers
3. **Training**: Adam optimizer with early stopping
4. **Validation**: Monitor validation loss

### Key Features
- ‚úÖ Parameter efficient
- ‚úÖ Faster training than LSTM
- ‚úÖ Similar performance to LSTM
- ‚úÖ Simpler architecture
- ‚ö†Ô∏è May struggle with very long sequences
- ‚ö†Ô∏è Less interpretable than LSTM
""")

## RNN Model
st.subheader("üîÑ RNN (Recurrent Neural Network)")
st.markdown("""
### Model Description
SimpleRNN is the basic recurrent neural network that processes sequential data through recurrent connections.

### Architecture Components
```
Hidden State: h‚Çú = tanh(Whh‚Çú‚Çã‚ÇÅ + Wx‚Çú + b)
Output: y‚Çú = Wyh‚Çú + by
```

### Training Process
1. **Data Preparation**: Create lagged sequences
2. **Architecture**: SimpleRNN + Dropout + Dense layers
3. **Training**: Adam optimizer
4. **Validation**: Monitor training/validation loss

### Key Features
- ‚úÖ Simple architecture
- ‚úÖ Fast training
- ‚úÖ Good for short sequences
- ‚úÖ Easy to understand
- ‚ö†Ô∏è Vanishing gradient problem
- ‚ö†Ô∏è Limited long-term memory
""")

## CNN Model
st.subheader("üñºÔ∏è CNN (Convolutional Neural Network)")
st.markdown("""
### Model Description
CNN uses convolutional layers to capture local temporal patterns in time series data.

### Architecture Components
```
Convolution: (f * x)[i] = Œ£‚±º f[j]x[i+j]
Pooling: Reduces dimensionality
Flatten: Converts to 1D for dense layers
Dense: Final prediction layer
```

### Training Process
1. **Data Preparation**: Create lagged sequences
2. **Architecture**: Conv1D + Dropout + Flatten + Dense
3. **Training**: Adam optimizer
4. **Validation**: Monitor training/validation loss

### Key Features
- ‚úÖ Captures local patterns
- ‚úÖ Translation invariant
- ‚úÖ Parameter sharing
- ‚úÖ Good for pattern recognition
- ‚ö†Ô∏è May miss long-term dependencies
- ‚ö†Ô∏è Requires appropriate kernel size
""")

# Model Comparison Section
st.header("üìä Model Comparison")
st.markdown("""
### Model Selection Guidelines

| Model Type | Best For | Strengths | Limitations |
|------------|----------|-----------|-------------|
| **ARIMA** | Linear trends, short-term forecasting | Interpretable, robust, automatic selection | Linear assumptions, limited complexity |
| **SARIMA** | Seasonal data, quarterly patterns | Captures seasonality, handles trends | More complex, requires seasonal data |
| **Exponential Smoothing** | Simple patterns, quick forecasts | Simple, interpretable, automatic selection | Limited to linear trends |
| **LSTM** | Complex patterns, long-term dependencies | Non-linear, long memory, robust | Requires more data, computationally intensive |
| **GRU** | Efficiency-focused applications | Parameter efficient, fast training | May struggle with very long sequences |
| **RNN** | Simple temporal patterns | Simple, fast, interpretable | Vanishing gradient, limited memory |
| **CNN** | Local pattern recognition | Good for local patterns, parameter sharing | May miss long-term dependencies |

### Performance Comparison Metrics
- **RMSE**: Root Mean Square Error (lower is better)
- **MAE**: Mean Absolute Error (lower is better)
- **MAPE**: Mean Absolute Percentage Error (lower is better)
- **R¬≤**: Coefficient of determination (higher is better)
- **AIC/BIC**: Model selection criteria (lower is better)

### Model Selection Strategy
1. **Start Simple**: Begin with ARIMA/Exponential Smoothing
2. **Check Seasonality**: Use SARIMA if strong seasonality detected
3. **Consider Complexity**: Use LSTM/GRU for complex patterns
4. **Validate Performance**: Compare models using multiple metrics
5. **Ensemble Approach**: Combine multiple models for robust forecasts
""")

# Evaluation Metrics Section
st.header("üîç Evaluation Metrics")
st.markdown("""
### Accuracy Metrics

#### RMSE (Root Mean Square Error)
```python
RMSE = ‚àö(Œ£(y·µ¢ - ≈∑·µ¢)¬≤ / n)
```
- **Interpretation**: Average prediction error in original units
- **Use Case**: Primary accuracy metric
- **Range**: 0 to ‚àû (lower is better)

#### MAE (Mean Absolute Error)
```python
MAE = Œ£|y·µ¢ - ≈∑·µ¢| / n
```
- **Interpretation**: Average absolute prediction error
- **Use Case**: Robust to outliers
- **Range**: 0 to ‚àû (lower is better)

#### MAPE (Mean Absolute Percentage Error)
```python
MAPE = (Œ£|y·µ¢ - ≈∑·µ¢| / |y·µ¢|) √ó 100 / n
```
- **Interpretation**: Average percentage error
- **Use Case**: Scale-independent comparison
- **Range**: 0% to ‚àû% (lower is better)

#### R¬≤ (Coefficient of Determination)
```python
R¬≤ = 1 - (Œ£(y·µ¢ - ≈∑·µ¢)¬≤ / Œ£(y·µ¢ - »≥)¬≤)
```
- **Interpretation**: Proportion of variance explained
- **Use Case**: Model fit assessment
- **Range**: 0 to 1 (higher is better)

### Model Selection Criteria

#### AIC (Akaike Information Criterion)
```python
AIC = 2k - 2ln(L)
```
- **Interpretation**: Balance between fit and complexity
- **Use Case**: Model comparison
- **Range**: -‚àû to ‚àû (lower is better)

#### BIC (Bayesian Information Criterion)
```python
BIC = ln(n)k - 2ln(L)
```
- **Interpretation**: Penalizes complexity more than AIC
- **Use Case**: Model comparison
- **Range**: -‚àû to ‚àû (lower is better)

### Diagnostic Tests

#### Residual Analysis
- **Normality Test**: Jarque-Bera test
- **Independence Test**: Ljung-Box test
- **Heteroskedasticity Test**: Breusch-Pagan test
- **Stationarity Test**: Augmented Dickey-Fuller test
""")

# Best Practices Section
st.header("üí° Best Practices")
st.markdown("""
### Data Preparation
1. **Handle Missing Values**: Remove or impute missing data appropriately
2. **Check for Outliers**: Identify and handle extreme values
3. **Ensure Consistency**: Maintain consistent data format and frequency
4. **Validate Data Quality**: Check for data integrity issues

### Model Selection
1. **Start Simple**: Begin with simpler models (ARIMA, Exponential Smoothing)
2. **Consider Data Characteristics**: Choose models based on data patterns
3. **Validate Assumptions**: Check model assumptions before proceeding
4. **Compare Multiple Models**: Use ensemble approaches when possible

### Training Process
1. **Proper Train-Test Split**: Use time-based splitting for time series
2. **Cross-Validation**: Implement appropriate validation strategies
3. **Hyperparameter Tuning**: Optimize parameters systematically
4. **Early Stopping**: Prevent overfitting in neural networks

### Evaluation
1. **Multiple Metrics**: Use various metrics for comprehensive evaluation
2. **Out-of-Sample Testing**: Always test on unseen data
3. **Residual Analysis**: Conduct thorough diagnostic testing
4. **Confidence Intervals**: Provide uncertainty quantification

### Deployment
1. **Model Monitoring**: Track model performance over time
2. **Regular Updates**: Retrain models with new data
3. **Documentation**: Maintain comprehensive model documentation
4. **Version Control**: Track model versions and changes
""")

# References Section
st.header("üìö References")
st.markdown("""
### Academic Papers
1. **ARIMA/SARIMA**: Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). Time Series Analysis: Forecasting and Control.
2. **Exponential Smoothing**: Holt, C. C. (2004). Forecasting seasonals and trends by exponentially weighted moving averages.
3. **LSTM**: Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory.
4. **GRU**: Cho, K., et al. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation.
5. **CNN for Time Series**: LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning.

### Software Libraries
- **Streamlit**: For web application framework
- **Pandas**: For data manipulation and analysis
- **NumPy**: For numerical computations
- **Scikit-learn**: For machine learning utilities
- **TensorFlow/Keras**: For deep learning models
- **Statsmodels**: For statistical models
- **PMDARIMA**: For automatic ARIMA selection
- **Plotly**: For interactive visualizations

### Data Sources
- **Malaysia Department of Statistics**: Quarterly Labour Force Survey Data
- **World Bank**: Economic indicators and employment statistics
- **International Labour Organization**: Global employment trends

### Additional Resources
- **Time Series Analysis**: Hyndman, R. J., & Athanasopoulos, G. (2018). Forecasting: principles and practice.
- **Deep Learning**: Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning.
- **Statistical Learning**: James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning.
""")

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666;'>
    <p>üìö Comprehensive Methodology Documentation | Malaysia Unemployment Forecasting</p>
    <p>Last updated: """ + datetime.now().strftime("%B %d, %Y") + """</p>
</div>
""", unsafe_allow_html=True) 